{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c331b3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Zebra mussels were first reported in the Great Lakes region circa 1989 and have since spread throughout much of the upper midwest. Understanding vectors and pathways of spread are critical for predicting and preventing the expansion of non-indigenous aquatic species (NAS). Overland transport via recreational watercraft trailered from infested lakes to new locations is the main vector for the spread of zebra and quagga mussels. However, an empirical understanding of how the number of public launches on a waterbody influences the overall risk for introduction for zebra/quagga mussels.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11335291",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "#### H) \n",
    "If zebra mussel presence is correlated with level of propagule pressure, then the number of boat ramps will impact the likelihood of introduction because overland transport is the main vector of spread for zebra/quagga mussels.\n",
    "\n",
    "#### Null) \n",
    "There is no correlation between the occurence of zebra/quagga and the numbers of public accesses.\n",
    "\n",
    "#### P) \n",
    "Logistic regression will indicate a correlation between the occurence of zebra/quagga and the numbers of public accesses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bbed8c",
   "metadata": {},
   "source": [
    "## Technical Approach\n",
    "\n",
    "The goal of this script is to test whether the number of public accesses on a waterbody is correlated with the presence of zebra/quagga mussels for MN. To do so, we will create a GIS pipeline using Python that calls to the USGS NAS Database API and uses helper functions to convert the retrieved data into a usable CSV format.  The code then spatially joins occurrence records pulled from the USGS NAS database API. Public accesses will then be joined to the vector layer and counted by lake. This geodataframe is then joined to the occurence records and a column is added for 'Present' where 1 indicates presence of zebra/quagga mussels and 0 indicates absence of zebra mussels. This final dataframe will then be split into testing and training datasets. A logistic regression model will be fit to the training data and model can then be used to predict to the testing dataset.  Accuracy scores can be produced for both training and testing datasets. Seaborn will be used to visualize the relationship between public accesses and the occurence of zebra/quagga mussels. Code to produce a confusion matrix and use the model to predict to a numpy array containing values for ramps between o and 100 is also provided.\n",
    "\n",
    "A secondary goal of this script is to be flexible such that the user can ask the same question for other NAS and within other states.  Instructions for modifying the code for a different NAS taxa and state are provided within the README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424acb4",
   "metadata": {},
   "source": [
    "## Download MN_lakes_shapefiles and Extract into Data Subdirectory\n",
    "https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHD/State/Shape/NHD_H_Minnesota_State_Shape.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f41d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import requests\n",
    "from functools import reduce\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c1046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions to convert USGS NAS Records into geodataframe.\n",
    "def _get_col_rename(df, dftype):\n",
    "    \"\"\"Returns a dictionary of columns to rename based on the dataframe and type('csv' or 'api')\"\"\"\n",
    "    \n",
    "    # Build a dictionary of column renamings for use in pandas rename function\n",
    "    renamed_columns = {}\n",
    "    column_names = list(df.columns)\n",
    "    lower_columns = [name.lower().replace(' ','').replace('_','') for name in column_names]\n",
    "    for i in range(len(column_names)):\n",
    "        renamed_columns[column_names[i]] = lower_columns[i]\n",
    "\n",
    "    if dftype == 'csv':\n",
    "        # build csv rename dictionary\n",
    "        renamed_columns['museumcatno'] = 'museumcatnumber'\n",
    "        renamed_columns['huc8number']  = 'huc8'\n",
    "    elif dftype == 'api':\n",
    "        # build api rename dictionary\n",
    "        renamed_columns['key']              = 'specimennumber'\n",
    "        renamed_columns['decimallatitude']  = 'latitude'\n",
    "        renamed_columns['decimallongitude'] = 'longitude'\n",
    "        renamed_columns['latlongsource']    = 'source'\n",
    "        renamed_columns['latlongaccuracy']  = 'accuracy'\n",
    "    else:\n",
    "        raise ValueError(f\"Dataframe type '{dftype}' invalid - Accepted inputs are 'csv' or 'api'\")\n",
    "\n",
    "    return renamed_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac409d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions to convert USGS NAS Records into geodataframe.\n",
    "def _manage_cols(df, drop_list=[], name_dict={}):\n",
    "    \"\"\"Private method for dropping and renaming columns in a dataframe, as well as creating one standard table from two different forms.\"\"\"\n",
    "\n",
    "    for colname in drop_list:\n",
    "        if colname not in df:\n",
    "            raise ValueError(f\"Can't drop column '{colname}' - '{colname}' does not exist in dataframe\")\n",
    "    for colname in list(name_dict.keys()):\n",
    "        if colname not in df:\n",
    "            raise ValueError(f\"Can't rename '{colname}' to '{name_dict[colname]}' - '{colname}' does not exist in dataframe\")\n",
    "        if colname in drop_list:\n",
    "            raise ValueError(f\"Can't rename '{colname}' to '{name_dict[colname]}' - '{colname}' in drop_list\")\n",
    "\n",
    "    column_names = np.setdiff1d(list(df.columns), list(name_dict.keys()))\n",
    "    lower_columns = [name.lower().replace(' ','').replace('_','') for name in column_names]\n",
    "    for i in range(len(column_names)):\n",
    "        name_dict[column_names[i]] = lower_columns[i]\n",
    "    \n",
    "    df = df.drop(drop_list, axis=1)\n",
    "    df = df.rename(columns=name_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06460894",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions to convert USGS NAS Records into geodataframe.\n",
    "URL_BASE = 'http://nas.er.usgs.gov/api/v2/'\n",
    "\n",
    "\n",
    "def api_df(species_id, limit, api_key):\n",
    "    \"\"\"Returns a pandas dataframe containing records about a species from the NAS database using their API\"\"\"\n",
    "    \n",
    "    # Check for API key\n",
    "    if api_key is not None:\n",
    "        url_request = f\"{URL_BASE}/occurrence/search?species_ID={species_id}&api_key={api_key}\"\n",
    "    else:\n",
    "        url_request = f\"{URL_BASE}/occurrence/search?species_ID={species_id}\"\n",
    "    \n",
    "    # Get dataframe from API request\n",
    "    request_json = requests.get(url_request, params={'limit':limit}).json()\n",
    "    api_df = pd.json_normalize(request_json, 'results')\n",
    "    api_df = _manage_cols(api_df)\n",
    "\n",
    "    # Add columns that are in a CSV dataframe but not an API dataframe\n",
    "    api_df['country']      = np.nan\n",
    "    api_df['drainagename'] = np.nan\n",
    "\n",
    "    # Rename columns\n",
    "    renamed_columns = _get_col_rename(api_df, 'api')\n",
    "    api_df = api_df.rename(columns=renamed_columns)\n",
    "\n",
    "    # Reorder columns\n",
    "    cols = list(api_df.columns)\n",
    "    cols = cols[0:8] + cols[33:34] + cols[8:33] + cols[34:] # country\n",
    "    cols = cols[0:16] + cols[34:] + cols[16:34] # drainagename\n",
    "    api_df = api_df[cols]\n",
    "    \n",
    "    return api_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a915737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run API function to get records\n",
    "zm = api_df(species_id = 5, limit = 10000, api_key = {\"speciesID\":5,\"itis_tsn\":81339,\"group\":\"Mollusks-Bivalves\",\n",
    "\"family\":\"Dreissenidae\",\"genus\":\"Dreissena\",\"species\":\"polymorpha\",\"subspecies\":\"\",\"variety\":\"\",\n",
    "\"authority\":\"(Pallas, 1771)\",\"common_name\":\"zebra mussel\",\"native_exotic\":\"Exotic\",\"Fresh/Marine/Brackish\":\"Freshwater\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ca3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only columns we want \n",
    "my_data = zm[[\"commonname\", \"state\", \"latitude\", \"longitude\", \"year\", \"status\", \"accuracy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dfb1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data \n",
    "my_data_fltr = my_data[(my_data['status'] == 'established') & (my_data['accuracy'] == 'Accurate')\n",
    "& (my_data['state'] == 'Minnesota')]  ## Replace with your state if using your own waterbody shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47042aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\PROGRA~1\\QGIS32~2.3\\apps\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\PROGRA~1\\QGIS32~2.3\\apps\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "# Add column for Present and set values for all rows to 1 \n",
    "my_data_fltr.loc[:,'Present'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "143e77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert csv to GeoDataFrame using lat/long columns to create point geometries\n",
    "user_data_gdf = gpd.GeoDataFrame(\n",
    "    my_data_fltr, geometry=gpd.points_from_xy(my_data_fltr.longitude, my_data_fltr.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89ec94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the crs (for some reason we need to set to a random crs and then convert to the crs we want for this to work)\n",
    "user_data_gdf.set_crs(4326, inplace = True, allow_override=True)\n",
    "my_data = user_data_gdf.to_crs(3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aa82d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commonname</th>\n",
       "      <th>state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>year</th>\n",
       "      <th>status</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Present</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>zebra mussel</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>46.746272</td>\n",
       "      <td>-92.124095</td>\n",
       "      <td>1989</td>\n",
       "      <td>established</td>\n",
       "      <td>Accurate</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-10255207.345 5900757.097)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>zebra mussel</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>43.993271</td>\n",
       "      <td>-91.443062</td>\n",
       "      <td>1992</td>\n",
       "      <td>established</td>\n",
       "      <td>Accurate</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-10179395.079 5464400.895)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>zebra mussel</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>44.098076</td>\n",
       "      <td>-91.707748</td>\n",
       "      <td>1992</td>\n",
       "      <td>established</td>\n",
       "      <td>Accurate</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-10208859.855 5480632.215)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>zebra mussel</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>44.160145</td>\n",
       "      <td>-91.810341</td>\n",
       "      <td>1992</td>\n",
       "      <td>established</td>\n",
       "      <td>Accurate</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-10220280.396 5490258.500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>zebra mussel</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>44.610962</td>\n",
       "      <td>-92.610870</td>\n",
       "      <td>1992</td>\n",
       "      <td>established</td>\n",
       "      <td>Accurate</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-10309394.931 5560481.929)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8097</th>\n",
       "      <td>zebra mussel</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>44.702140</td>\n",
       "      <td>-93.473200</td>\n",
       "      <td>2022</td>\n",
       "      <td>established</td>\n",
       "      <td>Accurate</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-10405389.027 5574750.782)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8099</th>\n",
       "      <td>zebra mussel</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>46.451390</td>\n",
       "      <td>-95.552640</td>\n",
       "      <td>2022</td>\n",
       "      <td>established</td>\n",
       "      <td>Accurate</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-10636871.229 5852982.217)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8100</th>\n",
       "      <td>zebra mussel</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>45.203610</td>\n",
       "      <td>-95.034590</td>\n",
       "      <td>2022</td>\n",
       "      <td>established</td>\n",
       "      <td>Accurate</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-10579202.167 5653632.872)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8101</th>\n",
       "      <td>zebra mussel</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>46.646680</td>\n",
       "      <td>-94.230750</td>\n",
       "      <td>2022</td>\n",
       "      <td>established</td>\n",
       "      <td>Accurate</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-10489719.107 5884592.741)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8104</th>\n",
       "      <td>zebra mussel</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>47.537470</td>\n",
       "      <td>-92.280930</td>\n",
       "      <td>2022</td>\n",
       "      <td>established</td>\n",
       "      <td>Accurate</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-10272666.138 6030248.394)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        commonname      state   latitude  longitude  year       status  \\\n",
       "23    zebra mussel  Minnesota  46.746272 -92.124095  1989  established   \n",
       "339   zebra mussel  Minnesota  43.993271 -91.443062  1992  established   \n",
       "340   zebra mussel  Minnesota  44.098076 -91.707748  1992  established   \n",
       "345   zebra mussel  Minnesota  44.160145 -91.810341  1992  established   \n",
       "357   zebra mussel  Minnesota  44.610962 -92.610870  1992  established   \n",
       "...            ...        ...        ...        ...   ...          ...   \n",
       "8097  zebra mussel  Minnesota  44.702140 -93.473200  2022  established   \n",
       "8099  zebra mussel  Minnesota  46.451390 -95.552640  2022  established   \n",
       "8100  zebra mussel  Minnesota  45.203610 -95.034590  2022  established   \n",
       "8101  zebra mussel  Minnesota  46.646680 -94.230750  2022  established   \n",
       "8104  zebra mussel  Minnesota  47.537470 -92.280930  2022  established   \n",
       "\n",
       "      accuracy  Present                           geometry  \n",
       "23    Accurate        1  POINT (-10255207.345 5900757.097)  \n",
       "339   Accurate        1  POINT (-10179395.079 5464400.895)  \n",
       "340   Accurate        1  POINT (-10208859.855 5480632.215)  \n",
       "345   Accurate        1  POINT (-10220280.396 5490258.500)  \n",
       "357   Accurate        1  POINT (-10309394.931 5560481.929)  \n",
       "...        ...      ...                                ...  \n",
       "8097  Accurate        1  POINT (-10405389.027 5574750.782)  \n",
       "8099  Accurate        1  POINT (-10636871.229 5852982.217)  \n",
       "8100  Accurate        1  POINT (-10579202.167 5653632.872)  \n",
       "8101  Accurate        1  POINT (-10489719.107 5884592.741)  \n",
       "8104  Accurate        1  POINT (-10272666.138 6030248.394)  \n",
       "\n",
       "[439 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your data\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c1eda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import waterbody shapefile, Get lakes larger than .5 km squared, change to projected crs and slice to just the lake name and geometry\n",
    "MN_lakes = gpd.read_file('./data/Shape/NHDWaterbody.shp')\n",
    "lake_mask = MN_lakes['areasqkm'] > .5\n",
    "lakes_filtered = MN_lakes.loc[lake_mask].to_crs(3857)\n",
    "lakes_gpd = lakes_filtered[['gnis_name', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b9a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Public Accesses shapefile, change to projected crs and slice to just the facility name and geometry\n",
    "access_df = gpd.read_file('./data/MN_accesses.shp')\n",
    "access_df_crs = access_df.to_crs(3857)\n",
    "my_accesses = access_df_crs[['FAC_NAME', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d8f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatial join Points to polygons/ Need to do by nearest because boat ramps might not intersect lake polygon\n",
    "#Use left join because we want to keep all the accesses and join them to the corresponding lake.\n",
    "accesses_with_lakes = gpd.sjoin_nearest(my_accesses, lakes_gpd, 'left', max_distance = 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slice to what we need\n",
    "# Rename the columns so we can group by right index (Lake_ID) Using the waterbody original index to groupby \n",
    "#ensures we aren't combining the count for ramps from lakes with same name.\n",
    "accesses_with_lakes_df = pd.DataFrame(accesses_with_lakes[['FAC_NAME', 'index_right']]).rename(columns = {'FAC_NAME' : \"ramps\", 'index_right' : \"lake_ID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a68cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby lake ID and count ramps per lake\n",
    "ramps = pd.DataFrame(accesses_with_lakes_df.groupby(['lake_ID']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb000a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-merge the ramps with the polygons \n",
    "ramps_df = gpd.GeoDataFrame(ramps.merge(lakes_gpd, left_index=True, right_index=True, how = 'right')).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join zm points to ramps layer\n",
    "positive_lakes = gpd.sjoin_nearest(my_data, ramps_df, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587076dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_lakes2 = positive_lakes.drop_duplicates('gnis_name').rename(columns = {'index_right' : \"lake_ID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lakes = positive_lakes2[['lake_ID', 'ramps', 'Present']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of the Lake IDs from positive lakes\n",
    "positive_lake_IDs = pos_lakes['lake_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9988923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for merging on index to get negative lakes\n",
    "ramps2 = pd.DataFrame(accesses_with_lakes_df.groupby(['lake_ID'], as_index=False).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d321e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lakes not in positive_lake_IDs list\n",
    "neg_lakes = ramps2[~ramps2['lake_ID'].isin(positive_lake_IDs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Present Column with 0's\n",
    "neg_lakes['Present'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate negative and positive dataframes.\n",
    "training_data = pd.concat([neg_lakes, pos_lakes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5854fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SLice to the columns we want and convert to numpy array \n",
    "my_model_data = training_data[['ramps', 'Present']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Scikit Learn function to split model data into training and testing datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "my_model_data[:,0].reshape(-1, 1), my_model_data[:,1], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Logistic Regression Model Variable\n",
    "logr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model to training data\n",
    "logr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb38894",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39cd238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression plot.\n",
    "sns.regplot(x=x_train, y=y_train, data= training_data, logistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict to testing dataset\n",
    "predictions = logr.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy score when predicting to training data\n",
    "logr.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ad7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Accuracy Score when predicting test data\n",
    "logr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does liklihood of invasion increase with an increase in ramps?\n",
    "log_odds = logr.coef_\n",
    "odds = np.exp(log_odds)\n",
    "print(odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pretty Confusion Matrix\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random test array\n",
    "my_test = np.arange(0.0, 100.0 , 1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model to predict test data\n",
    "logr.predict_proba(my_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b67f2",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Null hypothesis was not falsified.  \n",
    "\n",
    "Watercraft visits should also be included to better capture use of the accesses within the predictive model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc00786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
